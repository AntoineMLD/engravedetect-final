import logging
from datetime import datetime
from typing import Dict, Optional
from pathlib import Path
import scrapy
import sys
from ..database.reset_database import reset_database


class DataPipelineManager:
    """
    Orchestrateur principal du pipeline de donn√©es optiques.
    G√®re l'ensemble du flux de donn√©es : du scraping √† la cr√©ation des tables finales.
    """

    def __init__(self):
        self._setup_logging()
        self._setup_paths()
        self._load_dependencies()

    def _setup_logging(self) -> None:
        """Configure le syst√®me de logging."""
        log_format = "%(asctime)s [%(levelname)s] %(message)s"
        date_format = "%Y-%m-%d %H:%M:%S"

        # Cr√©er le dossier logs s'il n'existe pas
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)

        # Configurer le fichier de log avec la date
        log_file = logs_dir / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"

        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            datefmt=date_format,
            handlers=[
                logging.FileHandler(log_file, encoding="utf-8"),
                logging.StreamHandler(),
            ],
        )

        self.logger = logging.getLogger(__name__)

    def _setup_paths(self) -> None:
        """Configure les chemins du projet."""
        # Obtenir le chemin absolu du projet
        self.project_root = Path(__file__).resolve().parents[2]

        # Ajouter le chemin du projet au PYTHONPATH pour que Scrapy trouve les settings
        if str(self.project_root) not in sys.path:
            sys.path.insert(0, str(self.project_root))
            self.logger.info(f"Ajout du chemin du projet au PYTHONPATH: {self.project_root}")

    def _load_dependencies(self) -> None:
        """Charge les d√©pendances n√©cessaires."""
        try:
            from src.data.scraping.france_optique.spiders import (
                glass_spider,
                glass_spider_hoya,
                glass_spider_full_xpath,
                glass_spider_particular,
                glass_spider_optovision,
                glass_spider_indo_optical,
            )
            from src.data.processing.cleaner import OpticalDataCleaner

            self.spiders = {
                "base": glass_spider,
                "hoya": glass_spider_hoya,
                "full_xpath": glass_spider_full_xpath,
                "particular": glass_spider_particular,
                "optovision": glass_spider_optovision,
                "indo_optical": glass_spider_indo_optical,
            }

            self.cleaner = OpticalDataCleaner()
            self.logger.info("‚úÖ D√©pendances charg√©es avec succ√®s")

        except ImportError as e:
            self.logger.error(f"‚ùå Erreur lors du chargement des d√©pendances : {e}")
            raise

    def run_spiders(self) -> bool:
        """
        Lance tous les spiders pour collecter les donn√©es.

        Returns:
            bool: True si tous les spiders ont r√©ussi, False sinon
        """
        self.logger.info("üï∑Ô∏è D√©marrage des spiders...")

        try:
            from scrapy.crawler import CrawlerProcess
            from scrapy.utils.project import get_project_settings

            process = CrawlerProcess(get_project_settings())

            # Ajouter chaque spider au processus
            for spider_name, spider_module in self.spiders.items():
                self.logger.info(f"‚ûï Ajout du spider {spider_name}")
                # Obtenir la classe Spider directement du module
                spider_class = next(
                    obj
                    for name, obj in spider_module.__dict__.items()
                    if isinstance(obj, type) and issubclass(obj, scrapy.Spider)
                )
                process.crawl(spider_class)

            # Lancer tous les spiders
            process.start()

            self.logger.info("‚úÖ Tous les spiders ont termin√© avec succ√®s")
            return True

        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'ex√©cution des spiders : {e}")
            return False

    def export_staging_data(self) -> Optional[str]:
        """
        Exporte les donn√©es de la table staging en CSV.

        Returns:
            Optional[str]: Chemin du fichier CSV g√©n√©r√© ou None en cas d'erreur
        """
        self.logger.info("üì§ Export des donn√©es staging en CSV...")
        try:
            # Charger les donn√©es de staging
            df = self.cleaner.load_data_from_staging()
            if df.empty:
                self.logger.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e dans la table staging")
                return None

            # Exporter en CSV
            csv_path = self.cleaner.export_to_csv(df)
            self.logger.info(f"‚úÖ Donn√©es staging export√©es vers {csv_path}")
            return csv_path
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'export staging : {e}")
            return None

    def clean_and_enhance_data(self) -> bool:
        """
        Nettoie les donn√©es et les ins√®re dans la table enhanced.

        Returns:
            bool: True si le processus a r√©ussi, False sinon
        """
        self.logger.info("üßπ Nettoyage et am√©lioration des donn√©es...")
        try:
            # Charger et nettoyer les donn√©es
            df_raw = self.cleaner.load_data_from_staging()
            if df_raw.empty:
                self.logger.error("‚ùå Aucune donn√©e √† nettoyer dans la table staging")
                return False

            df_clean = self.cleaner.clean_dataframe(df_raw)
            if df_clean.empty:
                self.logger.error("‚ùå Aucune donn√©e valide apr√®s nettoyage")
                return False

            # Cr√©er la table enhanced et ins√©rer les donn√©es
            self.cleaner.create_enhanced_table()
            self.cleaner.insert_to_enhanced(df_clean)

            # Afficher les statistiques
            self.cleaner.get_data_statistics(df_clean)

            self.logger.info("‚úÖ Donn√©es nettoy√©es et am√©lior√©es avec succ√®s")
            return True
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors du nettoyage des donn√©es : {e}")
            return False

    def export_enhanced_data(self) -> Optional[str]:
        """
        Exporte les donn√©es de la table enhanced en CSV.

        Returns:
            Optional[str]: Chemin du fichier CSV g√©n√©r√© ou None en cas d'erreur
        """
        self.logger.info("üì§ Export des donn√©es enhanced en CSV...")
        try:
            # Charger les donn√©es directement depuis la table enhanced
            df = self.cleaner.load_data_from_enhanced()
            if df.empty:
                self.logger.warning("‚ö†Ô∏è Aucune donn√©e trouv√©e dans la table enhanced")
                return None

            # Exporter en CSV
            csv_path = self.cleaner.export_enhanced_to_csv(df)
            self.logger.info(f"‚úÖ Donn√©es enhanced export√©es vers {csv_path}")
            return csv_path
        except Exception as e:
            self.logger.error(f"‚ùå Erreur lors de l'export enhanced : {e}")
            return None

    def run_full_pipeline(self) -> Dict[str, bool]:
        """
        Ex√©cute l'ensemble du pipeline de donn√©es.

        Returns:
            Dict[str, bool]: Dictionnaire indiquant le statut de chaque √©tape
        """
        results = {
            "reset_database": False,
            "spiders": False,
            "clean_and_enhance": False,
            "export_enhanced": False,
        }

        try:
            # 1. R√©initialiser la base de donn√©es
            self.logger.info("üîÑ R√©initialisation de la base de donn√©es...")
            if not reset_database():
                self.logger.error("‚ùå √âchec de la r√©initialisation de la base de donn√©es")
                return results
            results["reset_database"] = True

            # 2. Lancer les spiders
            if not self.run_spiders():
                self.logger.error("‚ùå √âchec des spiders")
                return results
            results["spiders"] = True

            # 3. Nettoyer et enrichir les donn√©es
            if not self.clean_and_enhance_data():
                self.logger.error("‚ùå √âchec du nettoyage et de l'enrichissement des donn√©es")
                return results
            results["clean_and_enhance"] = True

            # 4. Exporter les donn√©es enrichies
            csv_path = self.export_enhanced_data()
            if not csv_path:
                self.logger.error("‚ùå √âchec de l'export des donn√©es enrichies")
                return results
            results["export_enhanced"] = True

            self.logger.info("‚ú® Pipeline termin√© avec succ√®s!")
            return results

        except Exception as e:
            self.logger.error(f"‚ùå Erreur dans le pipeline : {e}")
            return results


def main():
    """Point d'entr√©e principal du script."""
    try:
        pipeline = DataPipelineManager()
        results = pipeline.run_full_pipeline()

        # Afficher le r√©sum√© des r√©sultats
        print("\nR√©sum√© du pipeline :")
        print("=" * 50)
        for step, success in results.items():
            status = "‚úÖ" if success else "‚ùå"
            print(f"{status} {step}")

        # Retourner un code d'erreur si une √©tape a √©chou√©
        return 0 if all(results.values()) else 1

    except Exception as e:
        print(f"‚ùå Erreur critique : {e}")
        return 1


if __name__ == "__main__":
    exit(main())
