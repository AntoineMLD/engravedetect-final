# EngraveDetect

SystÃ¨me de gestion et d'analyse de donnÃ©es optiques. Ce projet permet de collecter, traiter et analyser les donnÃ©es de verres optiques, notamment les gravures nasales et les caractÃ©ristiques techniques.

## FonctionnalitÃ©s

- Scraping automatisÃ© des donnÃ©es de verres optiques
- Nettoyage et standardisation des donnÃ©es
- Stockage structurÃ© dans une base de donnÃ©es Azure SQL
- DÃ©tection automatique des caractÃ©ristiques des verres :
  - Gamme (Standard, Premium, etc.)
  - SÃ©rie et niveau
  - Hauteurs minimales et maximales
  - Traitements (Protection, Photochromique)
  - MatÃ©riaux et indices

## Structure du Projet

```
engravedetect/
â”œâ”€â”€ src/                           # Code source principal
â”‚   â”œâ”€â”€ data/                      # Gestion des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ scraping/             # Scripts de scraping
â”‚   â”‚   â”‚   â””â”€â”€ france_optique/   # Spider pour France Optique
â”‚   â”‚   â”œâ”€â”€ processing/           # Traitement des donnÃ©es
â”‚   â”‚   â”‚   â””â”€â”€ cleaner.py        # Nettoyage des donnÃ©es
â”‚   â”‚   â””â”€â”€ export/               # Export des donnÃ©es
â”‚   â”‚       â””â”€â”€ csv_export.py     # Export vers CSV
â”‚   â”œâ”€â”€ database/                 # Gestion de la base de donnÃ©es
â”‚   â”‚   â””â”€â”€ models/              # ModÃ¨les de donnÃ©es
â”‚   â”‚       â””â”€â”€ tables.py        # DÃ©finition des tables
â”‚   â”œâ”€â”€ orchestrator/            # Orchestration du pipeline
â”‚   â”‚   â””â”€â”€ pipeline_manager.py  # Gestionnaire de pipeline
â”‚   â””â”€â”€ utils/                   # Utilitaires communs
â”œâ”€â”€ tests/                       # Tests unitaires
â”‚   â””â”€â”€ utils/                  # Tests des utilitaires
â”œâ”€â”€ data/                        # DonnÃ©es exportÃ©es
â”œâ”€â”€ logs/                        # Logs d'exÃ©cution
â”œâ”€â”€ scripts/                     # Scripts utilitaires
â”‚   â””â”€â”€ run_spiders.py         # Lancement des spiders
â””â”€â”€ .env                        # Configuration
```

## Structure de la Base de DonnÃ©es

### Tables Principales

- `verres` : Table principale des verres
  - `id` : Identifiant unique
  - `nom` : Nom du verre
  - `variante` : Type de variante (STANDARD, COURT)
  - `hauteur_min` : Hauteur minimale en mm (14mm standard, 11mm court)
  - `hauteur_max` : Hauteur maximale en mm (14mm standard, 11mm court)
  - `indice` : Indice de rÃ©fraction
  - `gravure` : Code de gravure nasale
  - `protection` : PrÃ©sence de protection (UV, Blue)
  - `photochromic` : Verre photochromique
  - Relations avec les tables de rÃ©fÃ©rence

### Tables de RÃ©fÃ©rence

- `fournisseurs` : Liste des fournisseurs
- `materiaux` : Types de matÃ©riaux
- `gammes` : Gammes de produits
- `series` : SÃ©ries et leurs caractÃ©ristiques

## Installation

1. CrÃ©er un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

2. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

3. Configurer les variables d'environnement :
- Copier `.env.example` vers `.env`
- Remplir les informations suivantes :
  ```
  AZURE_SERVER=votre_serveur.database.windows.net
  AZURE_DATABASE=nom_base
  AZURE_USERNAME=utilisateur
  AZURE_PASSWORD=mot_de_passe
  ```

## Utilisation

1. Lancer le scraping des donnÃ©es :
```bash
python -m src.data.scraping.france_optique.run_spiders
```

2. Nettoyer et traiter les donnÃ©es :
```bash
python -m src.data.processing.cleaner
```

3. CrÃ©er et remplir les tables :
```bash
python -m src.database.models.tables
```

## Valeurs par DÃ©faut

Le systÃ¨me utilise les valeurs par dÃ©faut suivantes :
- Gamme : "STANDARD"
- SÃ©rie : "INCONNU"
- Variante : "STANDARD"
- Hauteur : 14mm (11mm pour les verres courts)
- Protection : "NON"
- Photochromic : "NON"
- MatÃ©riau : "ORGANIQUE"
- Indice : 1.5
- Fournisseur : "INCONNU"

## Maintenance

Pour mettre Ã  jour la base de donnÃ©es :
1. Les nouvelles donnÃ©es sont d'abord stockÃ©es dans la table `staging`
2. Puis nettoyÃ©es et enrichies dans la table `enhanced`
3. Enfin importÃ©es dans les tables finales avec les relations appropriÃ©es 

## Pipeline d'ExÃ©cution

Le projet utilise un orchestrateur centralisÃ© qui gÃ¨re l'ensemble du pipeline de donnÃ©es. Voici les Ã©tapes principales :

1. **Initialisation**
   - VÃ©rification de la configuration
   - CrÃ©ation des dossiers nÃ©cessaires
   - Configuration des logs

2. **Pipeline de DonnÃ©es**
   ```python
   python -m src.orchestrator.pipeline_manager
   ```
   
   Le pipeline exÃ©cute sÃ©quentiellement :
   - Scraping des donnÃ©es sources
   - Nettoyage et enrichissement
   - Import dans la base de donnÃ©es
   - Export des rÃ©sultats

3. **SystÃ¨me de Logs**
   - Les logs sont stockÃ©s dans le dossier `logs/`
   - Format : `YYYY-MM-DD_pipeline.log`
   - Niveaux de logs :
     - INFO : Progression normale
     - WARNING : ProblÃ¨mes non critiques
     - ERROR : Erreurs nÃ©cessitant attention
     - DEBUG : DÃ©tails d'exÃ©cution

4. **Gestion des Erreurs**
   - Reprise automatique en cas d'erreur non critique
   - Sauvegarde de l'Ã©tat en cas d'interruption
   - Notification des erreurs critiques

## Exemple de Log

```log
2024-01-20 10:15:23 INFO     DÃ©marrage du pipeline
2024-01-20 10:15:24 INFO     âœ… Configuration chargÃ©e
2024-01-20 10:15:25 INFO     ğŸ•·ï¸ Lancement du scraping
2024-01-20 10:20:15 INFO     âœ… Scraping terminÃ© : 150 verres collectÃ©s
2024-01-20 10:20:16 INFO     ğŸ§¹ DÃ©but du nettoyage des donnÃ©es
2024-01-20 10:21:00 WARNING  âš ï¸ 3 entrÃ©es avec indices manquants
2024-01-20 10:21:01 INFO     âœ… Nettoyage terminÃ©
2024-01-20 10:21:02 INFO     ğŸ’¾ Import dans la base de donnÃ©es
2024-01-20 10:22:00 INFO     âœ… Pipeline terminÃ© avec succÃ¨s
``` 