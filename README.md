# API REST Gestion des Verres Optiques

API REST pour la gestion des donnÃ©es de verres optiques, dÃ©veloppÃ©e avec FastAPI.

## FonctionnalitÃ©s

- âœ¨ Gestion des verres optiques (consultation, filtrage)
- ğŸ”’ Authentification JWT
- ğŸ“Š Statistiques et listes (fournisseurs, matÃ©riaux)
- ğŸ“ Documentation API (Swagger/OpenAPI)

## PrÃ©requis

- Python 3.10+
- PostgreSQL

## Installation

1. Cloner le repository :
```bash
git clone <repository-url>
cd engravedetect-final
```

2. CrÃ©er un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
# ou
.\venv\Scripts\activate  # Windows
```

3. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

4. Configurer les variables d'environnement :
```bash
cp .env.example .env
# Modifier .env avec vos paramÃ¨tres
```

## Configuration

Le fichier `.env` doit contenir :
```env
DATABASE_URL=postgresql://user:password@localhost:5432/db_name
SECRET_KEY=your-secret-key
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30
```

## Utilisation

1. DÃ©marrer l'API :
```bash
uvicorn src.api.main:app --reload
```

2. AccÃ©der Ã  :
- API : http://localhost:8000/api/v1
- Documentation : http://localhost:8000/docs
- Documentation alternative : http://localhost:8000/redoc

## Tests

ExÃ©cuter les tests :
```bash
pytest tests/ -v
```

## Documentation API

La documentation complÃ¨te de l'API est disponible via Swagger UI Ã  l'adresse `/docs` ou ReDoc Ã  `/redoc`.

### Points d'accÃ¨s principaux

- `GET /api/v1/verres` : Liste des verres avec filtrage
- `GET /api/v1/verres/{id}` : DÃ©tails d'un verre
- `GET /api/v1/verres/fournisseurs/list` : Liste des fournisseurs
- `GET /api/v1/verres/materiaux/list` : Liste des matÃ©riaux

## Authentification

L'API utilise l'authentification JWT. Pour accÃ©der aux endpoints protÃ©gÃ©s :

1. Obtenir un token :
```bash
curl -X POST "http://localhost:8000/api/v1/auth/token" \
     -H "Content-Type: application/x-www-form-urlencoded" \
     -d "username=user@example.com&password=password"
```

2. Utiliser le token :
```bash
curl "http://localhost:8000/api/v1/verres" \
     -H "Authorization: Bearer votre-token"
```

## Structure du Projet

```
engravedetect-final/
â”œâ”€â”€ src/
â”‚   â””â”€â”€ api/
â”‚       â”œâ”€â”€ core/          # Configuration, DB, sÃ©curitÃ©
â”‚       â”œâ”€â”€ models/        # ModÃ¨les SQLAlchemy
â”‚       â”œâ”€â”€ routes/        # Routes API
â”‚       â”œâ”€â”€ schemas/       # SchÃ©mas Pydantic
â”‚       â””â”€â”€ services/      # Logique mÃ©tier
â”œâ”€â”€ tests/                 # Tests
â”œâ”€â”€ .env                   # Variables d'environnement
â””â”€â”€ requirements.txt       # DÃ©pendances
```

# EngraveDetect

SystÃ¨me de gestion et d'analyse de donnÃ©es optiques. Ce projet permet de collecter, traiter et analyser les donnÃ©es de verres optiques, notamment les gravures nasales et les caractÃ©ristiques techniques.

## FonctionnalitÃ©s

- Scraping automatisÃ© des donnÃ©es de verres optiques
- Nettoyage et standardisation des donnÃ©es
- Stockage structurÃ© dans une base de donnÃ©es Azure SQL
- DÃ©tection automatique des caractÃ©ristiques des verres :
  - Gamme (Standard, Premium, etc.)
  - SÃ©rie et niveau
  - Hauteurs minimales et maximales
  - Traitements (Protection, Photochromique)
  - MatÃ©riaux et indices

## Structure du Projet

```
engravedetect/
â”œâ”€â”€ src/                           # Code source principal
â”‚   â”œâ”€â”€ data/                      # Gestion des donnÃ©es
â”‚   â”‚   â”œâ”€â”€ scraping/             # Scripts de scraping
â”‚   â”‚   â”‚   â””â”€â”€ france_optique/   # Spider pour France Optique
â”‚   â”‚   â”œâ”€â”€ processing/           # Traitement des donnÃ©es
â”‚   â”‚   â”‚   â””â”€â”€ cleaner.py        # Nettoyage des donnÃ©es
â”‚   â”‚   â””â”€â”€ export/               # Export des donnÃ©es
â”‚   â”‚       â””â”€â”€ csv_export.py     # Export vers CSV
â”‚   â”œâ”€â”€ database/                 # Gestion de la base de donnÃ©es
â”‚   â”‚   â””â”€â”€ models/              # ModÃ¨les de donnÃ©es
â”‚   â”‚       â””â”€â”€ tables.py        # DÃ©finition des tables
â”‚   â”œâ”€â”€ orchestrator/            # Orchestration du pipeline
â”‚   â”‚   â””â”€â”€ pipeline_manager.py  # Gestionnaire de pipeline
â”‚   â””â”€â”€ utils/                   # Utilitaires communs
â”œâ”€â”€ tests/                       # Tests unitaires
â”‚   â””â”€â”€ utils/                  # Tests des utilitaires
â”œâ”€â”€ data/                        # DonnÃ©es exportÃ©es
â”œâ”€â”€ logs/                        # Logs d'exÃ©cution
â”œâ”€â”€ scripts/                     # Scripts utilitaires
â”‚   â””â”€â”€ run_spiders.py         # Lancement des spiders
â””â”€â”€ .env                        # Configuration
```

## Structure de la Base de DonnÃ©es

### Tables Principales

- `verres` : Table principale des verres
  - `id` : Identifiant unique
  - `nom` : Nom du verre
  - `variante` : Type de variante (STANDARD, COURT)
  - `hauteur_min` : Hauteur minimale en mm (14mm standard, 11mm court)
  - `hauteur_max` : Hauteur maximale en mm (14mm standard, 11mm court)
  - `indice` : Indice de rÃ©fraction
  - `gravure` : Code de gravure nasale
  - `protection` : PrÃ©sence de protection (UV, Blue)
  - `photochromic` : Verre photochromique
  - Relations avec les tables de rÃ©fÃ©rence

### Tables de RÃ©fÃ©rence

- `fournisseurs` : Liste des fournisseurs
- `materiaux` : Types de matÃ©riaux
- `gammes` : Gammes de produits
- `series` : SÃ©ries et leurs caractÃ©ristiques

## Installation

1. CrÃ©er un environnement virtuel :
```bash
python -m venv venv
source venv/bin/activate  # Linux/Mac
venv\Scripts\activate     # Windows
```

2. Installer les dÃ©pendances :
```bash
pip install -r requirements.txt
```

3. Configurer les variables d'environnement :
- Copier `.env.example` vers `.env`
- Remplir les informations suivantes :
  ```
  AZURE_SERVER=votre_serveur.database.windows.net
  AZURE_DATABASE=nom_base
  AZURE_USERNAME=utilisateur
  AZURE_PASSWORD=mot_de_passe
  ```

## Utilisation

1. Lancer le scraping des donnÃ©es :
```bash
python -m src.data.scraping.france_optique.run_spiders
```

2. Nettoyer et traiter les donnÃ©es :
```bash
python -m src.data.processing.cleaner
```

3. CrÃ©er et remplir les tables :
```bash
python -m src.database.models.tables
```

## Valeurs par DÃ©faut

Le systÃ¨me utilise les valeurs par dÃ©faut suivantes :
- Gamme : "STANDARD"
- SÃ©rie : "INCONNU"
- Variante : "STANDARD"
- Hauteur : 14mm (11mm pour les verres courts)
- Protection : "NON"
- Photochromic : "NON"
- MatÃ©riau : "ORGANIQUE"
- Indice : 1.5
- Fournisseur : "INCONNU"

## Maintenance

Pour mettre Ã  jour la base de donnÃ©es :
1. Les nouvelles donnÃ©es sont d'abord stockÃ©es dans la table `staging`
2. Puis nettoyÃ©es et enrichies dans la table `enhanced`
3. Enfin importÃ©es dans les tables finales avec les relations appropriÃ©es 

## Pipeline d'ExÃ©cution

Le projet utilise un orchestrateur centralisÃ© qui gÃ¨re l'ensemble du pipeline de donnÃ©es. Voici les Ã©tapes principales :

1. **Initialisation**
   - VÃ©rification de la configuration
   - CrÃ©ation des dossiers nÃ©cessaires
   - Configuration des logs

2. **Pipeline de DonnÃ©es**
   ```python
   python -m src.orchestrator.pipeline_manager
   ```
   
   Le pipeline exÃ©cute sÃ©quentiellement :
   - Scraping des donnÃ©es sources
   - Nettoyage et enrichissement
   - Import dans la base de donnÃ©es
   - Export des rÃ©sultats

3. **SystÃ¨me de Logs**
   - Les logs sont stockÃ©s dans le dossier `logs/`
   - Format : `YYYY-MM-DD_pipeline.log`
   - Niveaux de logs :
     - INFO : Progression normale
     - WARNING : ProblÃ¨mes non critiques
     - ERROR : Erreurs nÃ©cessitant attention
     - DEBUG : DÃ©tails d'exÃ©cution

4. **Gestion des Erreurs**
   - Reprise automatique en cas d'erreur non critique
   - Sauvegarde de l'Ã©tat en cas d'interruption
   - Notification des erreurs critiques

## Exemple de Log

```log
2024-01-20 10:15:23 INFO     DÃ©marrage du pipeline
2024-01-20 10:15:24 INFO     âœ… Configuration chargÃ©e
2024-01-20 10:15:25 INFO     ğŸ•·ï¸ Lancement du scraping
2024-01-20 10:20:15 INFO     âœ… Scraping terminÃ© : 150 verres collectÃ©s
2024-01-20 10:20:16 INFO     ğŸ§¹ DÃ©but du nettoyage des donnÃ©es
2024-01-20 10:21:00 WARNING  âš ï¸ 3 entrÃ©es avec indices manquants
2024-01-20 10:21:01 INFO     âœ… Nettoyage terminÃ©
2024-01-20 10:21:02 INFO     ğŸ’¾ Import dans la base de donnÃ©es
2024-01-20 10:22:00 INFO     âœ… Pipeline terminÃ© avec succÃ¨s
``` 