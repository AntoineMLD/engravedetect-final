import logging
import os
from datetime import datetime
from typing import Dict, List, Optional
from pathlib import Path
import scrapy

class DataPipelineManager:
    """
    Orchestrateur principal du pipeline de donnÃ©es optiques.
    GÃ¨re l'ensemble du flux de donnÃ©es : du scraping Ã  la crÃ©ation des tables finales.
    """
    
    def __init__(self):
        self._setup_logging()
        self._load_dependencies()
        
    def _setup_logging(self) -> None:
        """Configure le systÃ¨me de logging."""
        log_format = "%(asctime)s [%(levelname)s] %(message)s"
        date_format = "%Y-%m-%d %H:%M:%S"
        
        # CrÃ©er le dossier logs s'il n'existe pas
        logs_dir = Path("logs")
        logs_dir.mkdir(exist_ok=True)
        
        # Configurer le fichier de log avec la date
        log_file = logs_dir / f"pipeline_{datetime.now().strftime('%Y%m%d_%H%M%S')}.log"
        
        logging.basicConfig(
            level=logging.INFO,
            format=log_format,
            datefmt=date_format,
            handlers=[
                logging.FileHandler(log_file, encoding='utf-8'),
                logging.StreamHandler()
            ]
        )
        
        self.logger = logging.getLogger(__name__)
        
    def _load_dependencies(self) -> None:
        """Charge les dÃ©pendances nÃ©cessaires."""
        try:
            from src.data.scraping.france_optique.spiders import (
                glass_spider_hoya,
                glass_spider_full_xpath,
                glass_spider_particular,
                glass_spider_optovision,
                glass_spider_indo_optical
            )
            from src.data.processing.cleaner import OpticalDataCleaner
            from src.database.models.tables import DatabaseManager
            
            self.spiders = {
                'hoya': glass_spider_hoya,
                'full_xpath': glass_spider_full_xpath,
                'particular': glass_spider_particular,
                'optovision': glass_spider_optovision,
                'indo_optical': glass_spider_indo_optical
            }
            
            self.cleaner = OpticalDataCleaner()
            self.db_manager = DatabaseManager()
            
            self.logger.info("âœ… DÃ©pendances chargÃ©es avec succÃ¨s")
            
        except ImportError as e:
            self.logger.error(f"âŒ Erreur lors du chargement des dÃ©pendances : {e}")
            raise
            
    def run_spiders(self) -> bool:
        """
        Lance tous les spiders pour collecter les donnÃ©es.
        
        Returns:
            bool: True si tous les spiders ont rÃ©ussi, False sinon
        """
        self.logger.info("ğŸ•·ï¸ DÃ©marrage des spiders...")
        
        try:
            from scrapy.crawler import CrawlerProcess
            from scrapy.utils.project import get_project_settings
            
            process = CrawlerProcess(get_project_settings())
            
            # Ajouter chaque spider au processus
            for spider_name, spider_module in self.spiders.items():
                self.logger.info(f"â• Ajout du spider {spider_name}")
                # Obtenir la classe Spider directement du module
                spider_class = next(obj for name, obj in spider_module.__dict__.items() 
                                  if isinstance(obj, type) and issubclass(obj, scrapy.Spider))
                process.crawl(spider_class)
            
            # Lancer tous les spiders
            process.start()
            
            self.logger.info("âœ… Tous les spiders ont terminÃ© avec succÃ¨s")
            return True
            
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de l'exÃ©cution des spiders : {e}")
            return False
            
    def export_staging_data(self) -> Optional[str]:
        """
        Exporte les donnÃ©es de la table staging en CSV.
        
        Returns:
            Optional[str]: Chemin du fichier CSV gÃ©nÃ©rÃ© ou None en cas d'erreur
        """
        self.logger.info("ğŸ“¤ Export des donnÃ©es staging en CSV...")
        try:
            # Charger les donnÃ©es de staging
            df = self.cleaner.load_data_from_staging()
            # Exporter en CSV
            csv_path = self.cleaner.export_to_csv(df)
            self.logger.info(f"âœ… DonnÃ©es staging exportÃ©es vers {csv_path}")
            return csv_path
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de l'export staging : {e}")
            return None
            
    def clean_and_enhance_data(self) -> bool:
        """
        Nettoie les donnÃ©es et les insÃ¨re dans la table enhanced.
        
        Returns:
            bool: True si le processus a rÃ©ussi, False sinon
        """
        self.logger.info("ğŸ§¹ Nettoyage et amÃ©lioration des donnÃ©es...")
        try:
            # Charger et nettoyer les donnÃ©es
            df_raw = self.cleaner.load_data_from_staging()
            df_clean = self.cleaner.clean_dataframe(df_raw)
            
            # CrÃ©er la table enhanced et insÃ©rer les donnÃ©es
            self.cleaner.create_enhanced_table()
            self.cleaner.insert_to_enhanced(df_clean)
            
            # Afficher les statistiques
            self.cleaner.get_data_statistics(df_clean)
            
            self.logger.info("âœ… DonnÃ©es nettoyÃ©es et amÃ©liorÃ©es avec succÃ¨s")
            return True
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors du nettoyage des donnÃ©es : {e}")
            return False
            
    def export_enhanced_data(self) -> Optional[str]:
        """
        Exporte les donnÃ©es de la table enhanced en CSV.
        
        Returns:
            Optional[str]: Chemin du fichier CSV gÃ©nÃ©rÃ© ou None en cas d'erreur
        """
        self.logger.info("ğŸ“¤ Export des donnÃ©es enhanced en CSV...")
        try:
            # Charger les donnÃ©es de la table enhanced
            df = self.cleaner.load_data_from_staging()  # On charge d'abord les donnÃ©es brutes
            df_clean = self.cleaner.clean_dataframe(df)  # On les nettoie
            # Exporter en CSV
            csv_path = self.cleaner.export_enhanced_to_csv(df_clean)
            self.logger.info(f"âœ… DonnÃ©es enhanced exportÃ©es vers {csv_path}")
            return csv_path
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de l'export enhanced : {e}")
            return None
            
    def create_main_tables(self) -> bool:
        """
        CrÃ©e les tables principales et y insÃ¨re les donnÃ©es.
        
        Returns:
            bool: True si le processus a rÃ©ussi, False sinon
        """
        self.logger.info("ğŸ—ï¸ CrÃ©ation des tables principales...")
        try:
            self.db_manager.process()
            self.logger.info("âœ… Tables principales crÃ©Ã©es avec succÃ¨s")
            return True
        except Exception as e:
            self.logger.error(f"âŒ Erreur lors de la crÃ©ation des tables principales : {e}")
            return False
            
    def run_full_pipeline(self) -> Dict[str, bool]:
        """
        ExÃ©cute l'ensemble du pipeline de donnÃ©es.
        
        Returns:
            Dict[str, bool]: Statut de chaque Ã©tape du pipeline
        """
        self.logger.info("ğŸš€ DÃ©marrage du pipeline complet...")
        self.logger.info("=" * 80)
        
        results = {
            "spiders": False,
            "staging_export": False,
            "data_cleaning": False,
            "enhanced_export": False,
            "main_tables": False
        }
        
        # 1. Lancer les spiders
        results["spiders"] = self.run_spiders()
        if not results["spiders"]:
            self.logger.error("â›” ArrÃªt du pipeline : Ã©chec des spiders")
            return results
            
        # 2. Exporter les donnÃ©es staging
        staging_csv = self.export_staging_data()
        results["staging_export"] = staging_csv is not None
        
        # 3. Nettoyer et amÃ©liorer les donnÃ©es
        results["data_cleaning"] = self.clean_and_enhance_data()
        if not results["data_cleaning"]:
            self.logger.error("â›” ArrÃªt du pipeline : Ã©chec du nettoyage des donnÃ©es")
            return results
            
        # 4. Exporter les donnÃ©es enhanced
        enhanced_csv = self.export_enhanced_data()
        results["enhanced_export"] = enhanced_csv is not None
        
        # 5. CrÃ©er les tables principales
        results["main_tables"] = self.create_main_tables()
        
        # RÃ©sumÃ© final
        self.logger.info("=" * 80)
        self.logger.info("ğŸ“Š RÃ©sumÃ© du pipeline :")
        for step, success in results.items():
            status = "âœ…" if success else "âŒ"
            self.logger.info(f"{status} {step}")
        
        return results

def main():
    """Point d'entrÃ©e principal du script."""
    try:
        pipeline = DataPipelineManager()
        results = pipeline.run_full_pipeline()
        
        # VÃ©rifier si toutes les Ã©tapes ont rÃ©ussi
        if all(results.values()):
            logging.info("âœ¨ Pipeline terminÃ© avec succÃ¨s !")
            return 0
        else:
            logging.error("âš ï¸ Pipeline terminÃ© avec des erreurs.")
            return 1
            
    except Exception as e:
        logging.error(f"âŒ Erreur fatale dans le pipeline : {e}")
        return 1

if __name__ == "__main__":
    exit_code = main()
    exit(exit_code) 